\chapter{The k-Means Clustering Algorithms}\label{chapter:kmeans}

In this chapter the data mining algorithm k-Means is presented. K-means is a well-studied clustering algorithms, partitioning a data set into $k$ clusters, where all data objects within a cluster are similar to each other and dissimilar to the data objects in the other clusters. The goal is to find the best clustering, minimizing the total squared distance between each data point and its assigned center. While the solution to that problem is NP-hard, there are several heuristics, in particular the Lloyd algorithm, which is a local search solution to this problem.

\section{Motivation}
 
We decided to use the k-Means algorithms as proof-of-concept algorithm for data mining in the HyPer database because it is an algorithm widely used and very popular. In fact, a survey of clustering data mining techniques in 2002 states that the algoritm “is by far the most popular clustering algorithm used in scientific and industrial applications”. K-means is also part of the 10 top data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM) in December 2006, next to other famous algorithms such as SVM and the PageRank algorithm. 
\\
Since the algorithm is very popular and easy to understand, makes it suitable for the first implementation of a data mining algorithm on HyPer. To our best knowledge, all major data mining tools are implementing k-Means, which makes it neat to compare regarding the running time. The cluster compactness, i.e. the sum of squared errors is a good quality measurement of the clustering, and allows very good comparability. 
Parts of K-means can be executed in parallel, and since HyPer supports parallel computation, it will be another interesting research to evaluate how single-threaded execution of K-means vs a multi-threaded computation. We are expecting tremendous performance gains executing HyPer on a multicore machine.

\section{The Lloyd Algorithms}

In the following section we discuss the Lloyed algorithm, usually referred as k-Means in literature and in this work as well. Formally, the k-Means problem and the k-Means algorithm are described the following: For a given integer $k$ and a data set of $n$ data points $X \subset  \mathbb{R}^d$, choose $k$ centers $C$ to minimize the sum of squared error function,
\begin{equation*}
sse = \Sigma_{x \in X} min_{c \in C} ||x - c||^2
\end{equation*}
By finding these center points, each data point is assigned to a center point and we have found our clustering.
\\ 
Solving such a problem is NP-hard, however Lloyd~cite proposed a local search solution usually resulting in good groupings. The algorithm starts with $k$ arbitrary center points, typically chosen at random from the data points. Then, each data point is assigned to the closest center, using a distance function. Usually, the euclidean distance is used, other implementations allow to choose between several distance functions. After assigning each data point to its closest center, the centers gets updated, i.e. the center is the mean coordinate of all the data points assigned to this center. Then the process begins again, assigning the data points again, now to the updated centers. This continues until the process stabilizes and the algorithm converges.
\\
The k-Means algorithms is described then the follwing:

\begin{enumerate} 
\item Arbitrarily choose $k$ centers $C = \{c_1, c_2, \cdots, c_k\}$ uniformly at random from $X$.
\item For each $i \in \{1, \cdots, k\}$, set the cluster $C_i$ to be the set of points in $X$ that are closer to cluster $c_i$ than to any other cluster $c_j$ for all $j \neq i$.
\item For each $i \in \{1, \cdots, k\}$, set $c_i$ as the center of all points assigned to $C_i$: 

\begin{equation*}
c_i = \frac{1}{|C_i|} \Sigma_{x \in C_i} x.
\end{equation*}

\item Repeat Steps 2 and 3 until $C$ no longer changes, i.e. the algorithm converges.
\end{enumerate}

This simple algorithm terminates in practice very fast and provides mostly good result.

\section{Example}

In this section the k-Means algorithm is presented by an example. Our example data set consists of five data points with two dimensions: $x_0(3,9), x_1(2,5), x_2(5,8), x_3(7,5), x_4(4,2)$. This data will be clustered using the k-Means algorithm, with $k = 2$, i.e. we are searching for two clusters. First, the algorithm starts with an initialization phase: Randomly, we take two instances as initial center points of our two clusters. In this example, these data points are $c_0(3,9)$ and $c_1(4,2)$. 
\\
Next, we start the first iteration of the k-Means algorithms and compute the euclidean distance from each data point to $c_0$ and to $c_1$. Each data point is then assigned to the closest cluster, as shown in~\autoref{tab:kmeans_iter_1}.

\begin{table}[htsb]
  \caption[Computations in Iteration 1]{Computations in Iteration 1.}\label{tab:kmeans_iter_1}
  \centering
  \begin{tabular}{l l l l}
    \toprule
      Data Point & $c_0(3,9)$ & $c_1(4,2)$ & Cluster \\
    \midrule
        $x_0(3,9)$ & 0.00 & 7.07 & $C_0$ \\
        $x_1(2,5)$ & 4.12 & 3.61 & $C_1$ \\
        $x_2(5,8)$ & 2.24 & 6.08 & $C_0$ \\
        $x_3(7,5)$ & 5.66 & 4.24 & $C_1$ \\
        $x_4(4,2)$ & 7.07 & 0.00 & $C_1$ \\
    \bottomrule
  \end{tabular}
\end{table}

Now, the centers have to be recalculated as the mean of all assigned data points. The result are the updated center points $c_0(4,8.5)$ and $c_1(4.33,4)$. These are used to compute the distances again for each data point as shown in~\autoref{tab:kmeans_iter_2}. As the data points are then assigned to the closest cluster. As the center points have changed, the distances to the centers changes and data points could be assigned to new clusters. In our example the data points are assigned to the same clusters as after Iteration 2. Therefore the algorithm converges and we have found the result.~\autoref{fig:kmeans_example} depicts the two clusters. Cluster $C_0$ with the center point $c_0$ is marked by blue data points and is positioned on the top of the chart, while Cluster $C_1$ with the center point $c_1$ is a broader cluster from the middle of the chart to the bottom.

\begin{table}[htsb]
  \caption[Computations in Iteration 2]{Computations in Iteration 2.}\label{tab:kmeans_iter_2}
  \centering
  \begin{tabular}{l l l l}
    \toprule
      Data Point & $c_0(4,8.5)$ & $c_1(4.33,4)$ & Cluster \\
    \midrule
        $x_0(3,9)$ & 1.12 & 5.17 & $C_0$ \\
        $x_1(2,5)$ & 4.03 & 2.54 & $C_1$ \\
        $x_2(5,8)$ & 1.12 & 4.06 & $C_0$ \\
        $x_3(7,5)$ & 4.61 & 2.85 & $C_1$ \\
        $x_4(4,2)$ & 6.50 & 2.02 & $C_1$ \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{figure}[htsb]
  \centering
  \includegraphics[scale=0.5, trim="0cm 1cm 0cm 0cm"]{figures/kmeans_example}
  \caption[k-Means Example Clustering]{k-Means Example Clustering.}\label{fig:kmeans_example}
\end{figure}




\section{The k-Means++ Initialization Strategy}

Even though the Lloyd algorithm provides good results in practice, a clustering can be arbitrarily bad for some data sets. Particularly, the random choosing of the initial cluster centers can lead to a bad grouping which cannot be changed during the clustering algorithms. Therefore,~\cite{kmeans++} proposes a variation to the k-Means initialization strategy, called k-Means++. The centers are still chosen randomly from the data points, but the data points are weighted according to their distance from the closest, already chosen center. Hence, the probability of choosing data points as center that are far away from each other increases, leading to enhancements in speed and accuracy of the clustering.
\\
Formally, k-Means++ can be described the following:
Let $D(x)$ the shortest distance from a data point the closest, already picked center. 

\begin{enumerate} 
\item Take the first center $c1$, chosen uniformly at random from $X$.
\item The next center $c_i$ is choosen from $x \in X$ with the propability 

\begin{equation*}
\frac {D(x)^2} {\Sigma_{x \in X} D(x)^2}.
\end{equation*}

\item Repeat Step 2 until all $k$ centers are selected.
\item Continue as in the standard k-Means algorithm.
\end{enumerate}

The authors call the weighting in Step 2 the $D^2$ weighting.
\\
Most clustering tools implement a k-Means++ initialization strategy for k-means, therefore we will also provide this for the k-Means algorithm on HyPer.

