\chapter{Conclusions and Future Work}\label{chapter:conclusion}
In this thesis we have presented a proof-of-concept implementation of the k-Means clustering algorithm as HyPer operator. This chapter provides conclusions of the presented work and addresses open challenges.

\section{Conclusions}
With main memory databases we are able to relax traditional database constraints and benefit from unprecedented performance gains. The main memory system HyPer can execute OLTP and OLAP queries on one system without the need of an additional data warehouse. Such a system provides the opportunities to execute data mining algorithms directly on the database, and therefore to perform real-time data analysis. 
\\
In this thesis, we presented several approaches to implement the k-Means clustering algorithm as a HyPer operator: A C++ driven approach, an LLVM driven approach and a parallel implementation. The implementation details can be used as building blocks for further data mining algorithms for HyPer.
\\
In extensive experiments we observed that the LLVM version is faster than the C++ version on all used data tests, and that our operator is able to compete with existing data mining solutions such as R, Julia and Weka. In fact, the LLVM approach and the R implementation are usually providing the best results for serial execution of the k-Means algorithm. Only for high dimensional data sets, the HyPer operator presents slightly inferior results and is outnumbered by Julia's and R's implementation. However, the HyPer k-Means operator also provides the opportunity for parallel execution: As data sets grow, the parallel implementation is faster than all the other tools, also on high dimensional data sets.
\\
In conclusion, we showed that the k-Means algorithm implemented as HyPer operator can compete with and even outperform existing solutions, therefore data mining using the HyPer main memory database is strongly practicable.

\section{Future Work}

Many open challenges remain for data mining in HyPer. In this section we discuss future work and begin with general improvements for the k-Means algorithm.
\\
The presented k-Means operator implementation has three input parameters: The cluster number $k$, the maximum number of iterations and a verbose option to output statistics about the algorithm, such as the center coordinates and the cluster compactness. For the future we want to enhance the capabilities of k-Means and make our algorithm more flexible. 
\\
One possibility is to allow the user to choose the distance function. Our k-Means operator implements the euclidean distance, however, for some problems other distance measurements are more appropriate. Therefore we should offer to choose among the manhattan, euclidean, minkowski and chebyshev distance as well as the cosine similarity in future implementations.
\\
Applying normalization before the k-Means clustering takes place can improve the final result. This can be either implemented as input parameter for the k-Means algorithm or even better as standalone operator since other data mining algorithms can benefit from a normalization too.
\\
For big data sets the execution time of the k-Means algorithm can be quite high. The reason is that the algorithm has to take many iterations until convergence. An enhancement is an earlier termination: Usually, k-Means terminates if the clusters do not change anymore. However, for large data sets this could be a performance problem if a few data points are still moving from cluster to cluster while the majority of data points remain constant. Nevertheless, an additional iteration of the algorithm is initiated. Therefore an input parameter could specify a tolerable change of data points. If the number of changing data points is underneath this threshold, the algorithm terminates even though there is a chance to find a slightly better clustering. For example Julia's k-Means algorithm is able to perform such an earlier termination depending on a tolerance threshold.
\\
Heterogeneous input data is another problem of our algorithm. Not all data sets consist only of numerical data. Often it is a mixture between numerical, binary and categorical attributes. An improvement is to first convert the binary and categorical attributes to numerical attributes, and then improve the distance and cost function to keep the different dimensions comparable, as proposed in k-Means Mixed~\parencite{ahmad2007kmean}.
\\
Instead of implementing a new distance and cost function the existing distance function could be extended by assigning a weight to each dimension. This is not only useful for mixed data sets where binary attributes have a different weight than numerical attributes, but also for homogeneous data sets. Apart from dimensions, instances can also be weighted differently since some are more significant than others.
\\
K-Means is usually implemented as the Lloyd~\parencite{Lloyd82} algorithm, resulting in reasonable results. However, the execution time can be improved implementing the Hartigan and Wong algorithm~\parencite{hartigan1979algorithm}. In contrast to the Lloyd algorithm it updates the centroids at any time a point is moved and makes time-saving choices for finding the closest cluster. Therefore also the HyPer k-Means operator could benefit from such an implementation.
\\
In our implementation the output of k-Means is a table with an additional column specifying the cluster identifier. Since this table is returned by the consume function, it can be used in further SQL statements. The statistical information however, such as the final center coordinates or the number of iterations is only printed to the console. Therefore this information is lost and cannot be used in further SQL statements. An improvement is to generate additional tables storing the statistical information of a clustering. In this tables more information can be stored in an accessible way, e.g. the final center coordinates, the distance from each data point to its closest center and all statistical information. Up to now HyPer is only able to push tuples to the next consume operator and cannot generate additional tables. Since this information is important for analyzing the k-Means result, an extension of the existing operator model is suggested: Almost all data mining algorithms produce several additional intermediate and final results, thus this is an important feature for computational data mining in databases.
\\
As a database system, HyPer makes already extensive use of index structures. Data mining algorithms can also benefit from index structures: A nearest-neighbor data structure could improve clustering high dimensional data and boost the performance even more.
\\
One of the main observations of Chapter~\ref{chapter:evaluation} is that the LLVM implementation is much faster than the C++ version on all performed experiments. Therefore, the center initialization using the k-Means++ method could benefit from a generated LLVM implementation instead of the current C++ implementation with slow~\texttt{getDistance} calls between the~\texttt{compile time} and the~\texttt{runtime system}.
\\
Moreover, not only the k-Means++ implementation could benefit from an LLVM implementation, but also the parallel execution. As we have shown, for small $k$ and medium size data sets the parallel execution is still slower than the LLVM implementation. The main reason are the calls from the~\texttt{runtime} to the~\texttt{compile time system}, as the C++ driven approach is the building block of the parallel implementation. Therefore, a parallel implementation in LLVM code could tremendously boost the performance of the algorithm. 
\\
Also the parallelization itself can be improved: Only the process of computing the distances is parallelized so far. While this is already a big performance improvement the algorithm might even benefit more from a parallel execution of the initialization process and of the recalculation of the cluster centers.
\\
Obviously more data mining algorithms have to be implemented in HyPer to enable a full data mining experience. Therefore, the existing k-Means algorithm can be used as a building block. A very similar partitional clustering algorithm is the k-Medoids algorithm~\parencite{medoid}. In contrary to k-Means it chooses data points as centers instead of the mean of the mass of all assigned data points. This makes the algorithm more robust against outliers. As implementation, the existing k-Means algorithm can be reused, only the way of computing center points after data point assignment has to be changed according to the requirements of k-Medoids. 
\\
In k-Means we assign each data object to one distinct cluster. However, objects may belong to several clusters: A next step might be to enhance the k-Means algorithm with a focus on fuzzy clustering. As a long term goal, we also plan to include more clustering algorithms as well as algorithms for mining frequent itemsets and association rules, classification and outlier detection.
\\
For finding a ready market in the data science community it will be necessary to provide a functional language for executing the k-Means operator and other data mining algorithms, since not all data scientists might be fluent in the SQL language. However, this is a challenge that goes beyond the implementation of a k-Means algorithm and is part of a larger project of enabling computational data mining in HyPer. For this project, not only a functional language has to be provided, but also a way to visualize the results of data mining algorithms. Therefore the HyPer system and its functional language must be able to connect with a 2D plotting library to produce publication-ready, quality figures in a variety of forms. An example for a 2D plotting library based on a functional language is the Python library matplotlib~\parencite{Hunter:2007}. 





