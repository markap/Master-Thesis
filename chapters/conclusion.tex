\chapter{Conclusions and Future Work}\label{chapter:conclusion}
In this work, we have presented a proof-of-concept implementation of the k-Means clustering algorithm as HyPer operator. This chapter provides conclusions of the presented work and addresses open challanges.

\section{Conclusions}
With main memory databases we are able to relax traditional database constraints and benefit from unprecedented performance gains. The main memory system HyPer can execute OLTP and OLAP queries on the same systems without the need of an additional data warehouse. Such a system provides the opportunities to execute data mining algorithms directly on the database to benefit from data locality, performance and the advantage of using only one system. 
\\
After discussing existing systems for data mining, we presented several approaches to implement the k-Means algorithm as a HyPer operator: A C++ driven approach, a LLVM driven approach and a parallel implementation.
\\
As result we make the observation that the LLVM version is faster than the C++ version on all data tests, and that our operator is able to compete with existing data mining solutions. In fact, the LLVM approach and the R implementation are usually providing the best results. Only for high dimensional data sets, the HyPer operator presents poor results and is outnumbered by Julia and R's implementation. However, the HyPer operator also provides the opportunity for parallel execution. As data sets grow, the parallel implementation is faster than all the other tools.
In conclusion, we showed that the k-Means algorithm implemented as HyPer operator can compete and even outperform existing solutions.

\section{Future Work}

Many open challenges remain in mining data in HyPer. In this section we discuss this future work and begin with general improvements for the k-Means algorithm.
\\
Our k-Means implementation can take up to three input parameters: The cluster number k, the maximum number of iterations and a verbose option to output statistics about the algorithm execution. For the future we want to enhance the behaviour of k-Means and make our algorithm more flexible. That can be done by allowing the user to choose a distance function. For now, k-Means implements a Euclidean distance, however, for some problems other distances are more appropriate. Therefore we should offer to use the Manhattan, Euclidean, Minkowski and Chebyshev distance as well as the cosine similarity.
\\
Applying normalization before data clustering can improve the final result. This can be either implemented as input parameter for the k-Means algorithm or even better as standalone operator since other data mining algorithms can benefit from a normalization too.
\\
For big data sets the execution time of the k-Means algorithms can be quite high. The reasons is that the iterations has to take many iterations until it converges. An enhancement is an earlier termination: Usually, k-Means terminates if the clusters do not change anymore. However, for large data sets this could be a problem if a few data points are moving from cluster to cluster while the majority of data points remains constant. Nevertheless, an additional iteration of the algorithms has to be initiated. Therefore an additional input parameter can specify a tolerable change of data points. Is the change of data points underneath this border, the algorithm terminates even though there is a slight chance to find a better clustering.
\\
Heterogenous data is another problem for our algorithms. No all data sets consist only of numeric data. Often it is a mixture between numeric, binary and categorical attributes. An improvement is to first convert the binary and categorical to numeric attributes, and then improve the distance and cost function to keep the different dimensions comparable, as proposed in k-Means Mixed.
\\
Instead of implementing a new cost function the existing distance function can be improved by assigning a weight to each dimension. This is not only useful for mixed data sets but also for numerical attributes only. Assigning weights is not only possible for dimensions - also instance can be weighted differently since some are more significant than others.
\\
K-Means is usually implemented as the Lloyd algorithm, resulting in reasonable results. However, the execution time can be improved implementing the Hartigan-Wong algorithm. In contrary to the Lloyd algorithm it updates the centroids at any time a point is moved and makes time-saving choices for finding the closest cluster.
\\

So far, the result of k-Means is a table with an additional column specifying the cluster number. Since this table is returned using the consume function, it can be used on further SQL statements. The statistical information however, such as the final center coordinates or the number of iterations is only printed to the console. Therefore this information is lost and cannot be used in further SQL statements. A way forward is to generate additional tables storing the statistical information of a clustering. In this tables more information can be stored in an accessible way, e.g. the final center coordinates, the distance from each data point to its closest center and all statistical information. Up to now HyPer is only able to push tuples to the next consume operator and cannot generate additional tables. Since this information is very valid to analyze the k-Means result an extension of the existing operator model is suggested. Almost all data mining algorithms produce several additional intermediate and final results, thus this is a big step for computational data mining on databases.
\\
HyPer is a database system that makes already extensive use of index structures. Therefore an option is to improve the k-Means algorithms, e.g. by using nearest neighbour data structures when clustering high dimensional data.
\\
As shown in the evaluation chapter the LLVM implementation is much faster than the C++ version. Therefore, the center initialization using the k-Means++ method could benefit from a pure LLVM implementation instead of a C++ implementation with slow getDistance call for the center selection process.
\\
We have also shown the advantages of parallel execution of the k-Means algorithms. However, for small k and medium size data sets the parallel execution is still slower than the LLVM implementation. One reason is that the C++ version was used as parallel version leading to many calls from the runtime to the compile time system. A parallel implementation in LLVM code could tremendously boost the performance of the algorithm. Also the parallelization itself can be improved: Only the process of computing the distances is parallelized so far. While this is already a big performance improvement the algorithm would even benefit more from a parallel execution of the initialization process and of the updating of the cluster centers.
\\
For finding a ready market in the data scientist community it will be necessary to provide a functional language for executing the k-Means algorithm since not all data scientist might be fluent in SQL scripting languages. However, this is a challenge that goes beyond the implementation of a k-Means algorithm as a language has to be found that adapts all the available SQL expressions and operators of HyPer.
\\
The next point goes in the same direction: Data scientists do not only want to use the output as a database table or in a data structure of the functional language but also in a visual way. Therefore the HyPer system and its functional language must be able to connect with a 2D plotting library to produce publication quality figures in a variety of forms. For the k-Means clustering algorithms this means a possibility to plot the data points, e.g. as a scatter plot. An example for a 2D plotting library based on a functional language is the Python library matplotlib. 
\\
To fulfil the aim to provide further algorithms for data mining, it is appropriate to use the existing k-Means algorithm as building block. A very similar partitional clustering algorithm is the k-Medoids algorithm. In contrary to k-Means it chooses data points as centers instead of the mean of the mass of all assigned data points. This makes the algorithm more robust for handling outliers. As implementation, the existing k-Means algorithm can be reused, only the way of computing center points after data point assignment has to change according to the requirements of k-Medoids. 
\\
So far we assign each object to one distinct cluster. However, objects may belong to several clusters. The Expectationâ€“maximization (EM) algorithm is very similar to the k-Means algorithm and has a focus on fuzzy clustering. As a long term goal, we want to include also algorithm for mining frequent patterns and association rules, classification and outlier detection. 
information.










