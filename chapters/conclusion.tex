\chapter{Conclusions and Future Work}\label{chapter:conclusion}
In this work, we have presented a proof of concept implementation of the k-Means clustering algorithm as HyPer operator. This chapter provides conclusions of the presented work and addresses open challenges.

\section{Conclusions}
With main memory databases we are able to relax traditional database constraints and benefit from unprecedented performance gains. The main memory system HyPer can execute OLTP and OLAP queries on one systems without the need of an additional data warehouse. Such a system provides the opportunities to execute data mining algorithms directly on the database to benefit from data locality and high performance.
\\
After discussing existing systems for data mining, we presented several approaches to implement the k-Means algorithm as a HyPer operator: A C++ driven approach, a LLVM driven approach and a parallel implementation.
\\
As result we make the observation that the LLVM version is faster than the C++ version on all data tests, and that our operator is able to compete with existing data mining solutions. In fact, the LLVM approach and the R implementation are usually providing the best results. Only for high dimensional data sets, the HyPer operator presents poor results and is outnumbered by Julia and R's implementation. However, the HyPer operator also provides the opportunity for parallel execution. As data sets grow, the parallel implementation is faster than all the other tools, also on high dimensional data sets.
\\
In conclusion, we showed that the k-Means algorithm implemented as HyPer operator can compete and even outperform existing solutions, therefore data mining using the HyPer main memory database is strongly practicable.

\section{Future Work}

Many open challenges remain for data mining in HyPer. In this section we discuss future work and begin with general improvements for the k-Means algorithm.
\\
Our k-Means implementation can take up to three input parameters: The cluster number $k$, the maximum number of iterations and a verbose option to output statistics about the algorithm, such as the center coordinates and the cluster compactness. For the future we want to enhance the capabilities of k-Means and make our algorithm more flexible. 
\\
One possibility is to allow the user to choose a distance function. Our k-Means implements the euclidean distance, however, for some problems other distances are more appropriate. Therefore we should offer to choose among the manhattan, euclidean, minkowski and chebyshev distance as well as the cosine similarity.
\\
Applying normalization before data clustering can improve the final result. This can be either implemented as input parameter for the k-Means algorithm or even better as standalone operator since other data mining algorithms can benefit from a normalization too.
\\
For big data sets the execution time of the k-Means algorithm can be quite high. The reasons is that the iterations has to take many iterations until it converges. An enhancement is an earlier termination: Usually, k-Means terminates if the clusters do not change anymore. However, for large data sets this could be a problem if a few data points are moving from cluster to cluster while the majority of data points remains constant. Nevertheless, an additional iteration of the algorithm is initiated. Therefore an additional input parameter can specify a tolerable change of data points. If the number of changing data points is underneath this threshold, the algorithm terminates even though there is a chance to find a slightly better clustering.
\\
Heterogeneous data is another problem for our algorithm. No all data sets consist only of numerical data. Often it is a mixture between numerical, binary and categorical attributes. An improvement is to first convert the binary and categorical attributes to numerical attributes, and then improve the distance and cost function to keep the different dimensions comparable, as proposed in k-Means Mixed~\parencite{ahmad2007kmean}.
\\
Instead of implementing a new cost function the existing distance function can be improved by assigning a weight to each dimension. This is not only useful for mixed data sets but also for numerical attributes. Apart from dimensions instance can be also weighted differently since some are more significant than others.
\\
K-Means is usually implemented as the Lloyd~\parencite{Lloyd82} algorithm, resulting in reasonable results. However, the execution time can be improved implementing the Hartigan-Wong algorithm~\parencite{hartigan1979algorithm}. In contrast to the Lloyd algorithm it updates the centroids at any time a point is moved and makes time-saving choices for finding the closest cluster.
\\
In our implementation the output of k-Means is a table with an additional column specifying the cluster number. Since this table is returned by the consume function, it can be used in further SQL statements. The statistical information however, such as the final center coordinates or the number of iterations is only printed to the console. Therefore this information is lost and cannot be used in further SQL statements. An improvement is to generate additional tables storing the statistical information of a clustering. In this tables more information can be stored in an accessible way, e.g. the final center coordinates, the distance from each data point to its closest center and all statistical information. Up to now HyPer is only able to push tuples to the next consume operator and cannot generate additional tables. Since this information is very valid to analyze the k-Means result an extension of the existing operator model is suggested. Almost all data mining algorithms produce several additional intermediate and final results, thus this is an important feature for computational data mining in databases.
\\
HyPer is a database system that makes already extensive use of index structures. An option to improve the k-Means algorithm is by using nearest neighbor data structures when clustering high dimensional data. This could boost the performance even more.
\\
As shown in Chapter~\ref{chapter:evaluation} the LLVM implementation is much faster than the C++ version. Therefore, the center initialization using the k-Means++ method could benefit from a pure LLVM implementation instead of a C++ implementation with slow~\texttt{getDistance} calls for the center selection process.
\\
Not only the k-Means++ implementation could benefit from an LLVM implementation, but also the parallel execution. As we have shown, for small k and medium size data sets the parallel execution is still slower than the LLVM implementation. One reason is that the C++ version was used as basis of the parallel version leading to many calls from the~\texttt{runtime} to the~\texttt{compile time system}. A parallel implementation in LLVM code could tremendously boost the performance of the algorithm. 
\\
Also the parallelization itself can be improved: Only the process of computing the distances is parallelized so far. While this is already a big performance improvement the algorithm would even benefit more from a parallel execution of the initialization process and of the updating of the cluster centers.
\\
Obviously more data mining algorithms have to be implemented in HyPer to enable a full data mining experience. Therefore, the existing k-Means algorithm can be used as a building block. A very similar partitional clustering algorithm is the k-Medoids algorithm~\parencite{medoid}. In contrary to k-Means it chooses data points as centers instead of the mean of the mass of all assigned data points. This makes the algorithm more robust for handling outliers. As implementation, the existing k-Means algorithm can be reused, only the way of computing center points after data point assignment has to change according to the requirements of k-Medoids. 
\\
So far we assign each object to one distinct cluster. However, objects may belong to several clusters. A next step could be to enhance the k-Means algorithm with a focus on fuzzy clustering. As a long term goal, we also plan to include algorithms for mining frequent patterns and association rules, classification and outlier detection.
\\
For finding a ready market in the data science community it will be necessary to provide a functional language for executing the k-Means algorithm since not all data scientist might be fluent in the SQL language. However, this is a challenge that goes beyond the implementation of a k-Means algorithm as a language has to be found that adapts all the available SQL expressions and operators of HyPer.
\\
Moreover, data scientists do not only want to use a functional language, they also have to visualize the result of data mining algorithms. Therefore the HyPer system and its functional language must be able to connect with a 2D plotting library to produce publication quality figures in a variety of forms. For the k-Means clustering algorithms this means a possibility to plot the data points, e.g. as a scatter plot. An example for a 2D plotting library based on a functional language is the Python library matplotlib~\parencite{Hunter:2007}. 





