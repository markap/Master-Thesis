\chapter{Conclusions and Future Work}\label{chapter:conclusion}
In this thesis we have presented a proof-of-concept implementation of the k-Means clustering algorithm as HyPer operator. This chapter provides conclusions of the presented work and addresses open challenges.

\section{Conclusions}
With main memory databases we are able to relax traditional database constraints and benefit from unprecedented performance gains. The main memory system HyPer can execute OLTP and OLAP queries on one system without the need of an additional data warehouse. Such a system provides the opportunities to execute data mining algorithms directly on the database, and therefore to perform real-time data analysis. 
\\
In this thesis, we presented several approaches to implement the k-Means clustering algorithm as proof-of-concept algorithm for data mining in HyPer: A C++ driven approach, an LLVM driven approach and a parallel implementation have been presented. The implementation details can be used as building blocks for further data mining algorithms.
\\
In extensive experiments we observed that the LLVM version is faster than the C++ version on all used data tests, and that our operator is able to compete with existing data mining solutions such as R, Julia and Weka. In fact, the LLVM approach and the R implementation are usually providing the best results for serial execution of the k-Means algorithm. Only for high dimensional data sets, the HyPer operator presents slightly inferior results. However, the HyPer k-Means operator also provides the opportunity for parallel execution: As data sets grow, the parallel implementation is faster than all the other tools, also on high dimensional data sets.
\\
In conclusion, we showed that the k-Means algorithm implemented as HyPer operator can compete with and even outperform existing solutions, therefore data mining using the HyPer main memory database is strongly practicable.

\section{Future Work}

Many open challenges remain for data mining in HyPer. In this section we discuss future work and begin with general improvements for the k-Means algorithm.
The presented k-Means operator implementation has three input parameters: The cluster number $k$, the maximum number of iterations and a verbose option to output statistics of the performed execution, such as the center coordinates and the cluster compactness. 
In future implementations we want to enhance the capabilities of k-Means and make our algorithm more flexible. 
One approach is to allow the user to choose the distance function. So far, the k-Means operator implements the euclidean distance, however, for some problems, other distance measurements are more appropriate, such as the manhattan, euclidean, minkowski or chebyshev distance, or the cosine similarity.
\\
Applying normalization before performing the k-Means clustering is often necessary to get a reasonable good result. A normalization can be either implemented within the k-Means operator or even better as standalone operator so that other data mining algorithms can benefit from it too.
\\
For large data sets the execution time of the k-Means algorithm can be quite high since it might take many iterations until convergence. Usually, k-Means terminates when none of the clusters is changing anymore. However, for large data sets this could take a while, in particular when a few data points are still moving from cluster to cluster while the majority of data points remains constant. Nevertheless, an additional iteration of the algorithm is initiated, affecting the performance of the algorithm. An advancement is an input parameter specifying a tolerable change of data points, as realized in Julia's k-Means implementation. If the number of changing data points is underneath this tolerance threshold, the algorithm terminates even though there is a chance to find a slightly better clustering.
\\
Heterogeneous input data is another issue of the k-Means algorithm. Not all data sets consist only of numerical data. Often it is a mixture between numerical, binary and categorical attributes. An approach is to first convert the binary and categorical attributes to numerical attributes, and then provide an extended distance and cost function to keep the different dimensions comparable, as proposed in k-Means Mixed~\parencite{ahmad2007kmean}. Another option is to assign a weight to each dimension after the conversion, to deal with the different characteristics of binary and categorical attributes. This is not only useful for mixed data sets, but also for homogeneous numerical data sets.
\\
K-Means is usually implemented as the Lloyd~\parencite{Lloyd82} algorithm, resulting in reasonable good results. However, the execution time can be improved implementing the Hartigan and Wong algorithm~\parencite{hartigan1979algorithm}. In contrast to the Lloyd algorithm it updates the center points at any time a data point is moved and makes time-saving choices for finding the closest cluster. Therefore, such an implementation might improve the performance of the k-Means operator even more.
\\
The result of the HyPer k-Means clustering is currently presented as a relational table with an additional column specifying the cluster identifier. Since this table is returned by the~\texttt{consume} function, it can be used in further SQL statements. The statistical information however, such as the final center coordinates or the number of iterations, is only printed to the console. Therefore this information is lost and cannot be used in further analytical queries. An improvement is to generate additional relational tables storing the statistical information of the performed clustering. However, up to now HyPer is only capable of pushing tuples to the next consume operator and not of generating additional tables. Therefore, an extension of the existing operator model is suggested: Almost all data mining algorithms produce several intermediate and final results, thus the creation of additional tables is an important feature for computational data mining in HyPer.
\\
As database system, HyPer makes already extensive use of index structures, e.g. for efficient lookup and sorting. Data mining algorithms can also benefit from index structures: A nearest-neighbor data structure could improve clustering high dimensional data and boost the performance even more.
\\
One of the main observations of Chapter~\ref{chapter:evaluation} is that the LLVM implementation is much faster than the C++ version on all performed experiments. Therefore, the center initialization using the k-Means++ algorithm could benefit from a generated LLVM implementation instead of the current C++ implementation with slow~\texttt{getDistance} calls between the~\texttt{compile time} and the~\texttt{runtime system}. Moreover, not only the k-Means++ implementation can benefit from an LLVM implementation, but also the parallel execution. As we have shown, for small $k$ and medium size data sets the parallel execution is still slower than the LLVM implementation. The main reason are the calls from the~\texttt{runtime} to the~\texttt{compile time system}, as the C++ driven approach is the building block of the parallel implementation. Therefore, a parallel implementation in LLVM code could tremendously boost the performance of the algorithm. 
\\
Furthermore the parallelization itself can be improved: Only the process of computing the distances is parallelized so far. While this is already a big performance improvement the algorithm might even benefit more from a parallel execution of the initialization process and of the recalculation of the cluster centers.
\\
Obviously more data mining algorithms have to be implemented in HyPer to enable a full data mining experience. Therefore, the existing k-Means algorithm can be used as a building block. A similar partitional clustering algorithm is the k-Medoids algorithm~\parencite{medoid}. In contrary to k-Means it chooses data points as centers instead of the mean of the mass of all assigned data points. This makes the algorithm more robust against outliers. For implementation, the existing k-Means algorithm can be reused, only the way of computing center points after data point assignment has to be changed according to the requirements of k-Medoids. As long term goal, we plan to include more clustering algorithms as well as algorithms for mining frequent itemsets and association rules, classification and outlier detection.
\\
For finding a ready market in the data science community it will not be enough to provide HyPer with state-of-the-art data mining algorithms; it will also be necessary to provide a functional language for invoking the k-Means operator and other data mining algorithms, since not all data scientists might be fluent in the SQL language. Moreover, not only a functional language has to be provided, but also a way to visualize the results of data mining algorithms: The HyPer system and its functional language must be able to connect with a 2D plotting library to produce publication-ready, quality figures in a variety of forms. An example for a 2D plotting library based on a functional language is the Python library matplotlib~\parencite{Hunter:2007}. 





