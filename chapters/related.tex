\chapter{Related Systems}\label{chapter:related}

In this chapter we present state-of-the-art data mining software, frameworks and languages, their main characteristics and areas of application. We try to give a broad overview starting with research tools and dynamic languages, big data solutions, software systems that strongly integrate with existing databases, and finally database processing of data mining algorithms.

\section{Research Frameworks}
In this section we discuss two data mining tools - Weka~\parencite{Hall:2009:WDM:1656274.1656278} and ELKI~\parencite{DBLP:conf/ssdbm/AchtertKZ08} - that are strongly originated in the research community and are developed at universities. Additionally, Weka is often used for teaching data mining techniques. Nevertheless, both tools, in particular the more mature Weka, are also used in practice by industrial scientists.
\\
Weka is a data mining~\enquote{workbench} developed at the University of Waikato that gained big attention in the machine learning and data mining community over the past decade. It provides a large collection of algorithms for all aspects of data mining such as data preprocessing, classification, regression, clustering, association rule mining and visualization into one framework. Weka is written in Java and has with the~\enquote{Weka Explorer} a built-in GUI, but can also be executed directly within Java programs or from the command line. 
\\
Weka's default file format is the arff format, an ASCII text file that describes instances and its attributes. In contrast to csv files, arff files can be read incrementally by Weka, because the header of an arff file already determines whether a column is numeric or nominal. In contrast, all instances have to be inspected when working with csv files. Therefore, arff files provide performance improvements and high accuracy about the attributes' data type.
\\
ELKI (Environment for Developing KDD-Applications Supported by Index Structures) is a relatively new data mining framework developed at the Ludwigs-Maximilians-Universität München since 2009 with an even stronger focus on research. It's main objective is to enable a fair comparison of data mining algorithms based on experimental evaluation. In the strongly evolving field of data mining, many algorithms are released every year, often never seen again once the paper is published. It is common that the comparison of algorithms is based on experimental evaluation, which is not always comprehensible and reproducible. Moreover, the used code is not always published or properly documented. ELKI addresses these challenges and provides an implementation framework for new data mining algorithms, leading to a better comparability among them and therefore to a fairer evaluation of the newly proposed algorithms. 
\\
As Weka, ELKI is written in Java and can be either used via graphical user interface, in Java programs or the command line. A major strength of the framework is that it is able to read arbitrary data types and the support of any distance or similarity measure. Therefore, all kinds of complex data types can be integrated into existing algorithms, merely by providing a distance function for the given data type. ELKI also encourages the use of index structures to achieve performance gains when working with high dimensional data sets. 
\\
In contrast to Weka and ELKI, 


\section{Dynamic Languages for Statistical Computing}

While Weka and ELKI provide an excellent suite for data mining, data scientists often prefer a more dynamic language over sometimes bulky Java code. In particular for exploratory data analysis, dynamic scripting languages are very popular to get a first hands-on-feeling of the given data. Special purpose languages exist, with a syntax and built-in data structures that make data analysis tasks fast and concise. This section presents the R~\parencite{R/stats} language which is popular for decades, the previously announced Julia~\parencite{DBLP:journals/corr/abs-1209-5145} language and the Python ecosystem SciPy~\parencite{scipy}. R and Julia are both special purpose languages designed for data analysis, while Python is a general purpose programming language. However, the SciPy ecosystem provides excellent tools for data mining with a similar syntax and data structures as in R and Julia.
\\
R is a mature language for statistical computing and data analysis with a rich set of graphical techniques that makes it often researcher's number one tool for creating graphics for publications. Since R is a programming language most of the R libraries are written in R itself. However, C, C++ and Fortran code can be linked and called at run time and is often used for computationally intensive tasks. For advanced usage it is also possible to manipulate R objects directly via C code.
\\
Instead of just being a programming language, R depicts itself as an environment for statistical techniques: It is highly extensible via packages and provides many data mining algorithms, available via the CRAN\footnote{http://cran.r-project.org/, 15.03.2015.} network. RStudio\footnote{http://www.rstudio.com/, 15.03.2015.} offers an IDE for R programmers, and software products like JGR\footnote{http://www.rforge.net/JGR/, 15.03.2015.} provide a GUI for R programs.
\\
Thanks to the underlying C, C++ and Fortran libraries, R can do basic computations like matrix math very efficient and fast. However, the R language interpreter is rather slow, which discourages writing large libraries or complex abstractions in the R languages itself. Instead, the described low-level languages are used.
\\
Julia is a modern dynamic programming language for scientific computing, designed to address some of these concerns. As R, Julia is a programming language, written itself mainly in Julia, using C and Fortran for computationally expensive tasks. In contrast to R, Julia uses an LLVM-based JIT compilation for interpreting its own language. This results in stunning performance results: While the running time for basic language features such as matrix and vector computation does not differ to R, since both are using low-level languages, implementations in the Julia language are much faster and often comparable to C\footnote{http://julialang.org/benchmarks/, 15.03.2015.}. The same compilation technique is used in HyPer, therefore it will be interesting to compare both tools.
\\
For data analysis, external packages are available providing state-of-the-art data mining algorithms. However, it cannot compete with R's CRAN, one of the most impressive collections of statistical libraries available anywhere. Since Julia is a young language with a very active community and a growing popularity, we can expect many more algorithms and tools in the future.
\\
SciPy is a Python-based ecosystem enhancing the Python language into a data analysis environment. It's core packages are NumPy, SciPy, matplotlib and pandas. The NumPy library provides matrix data structures and efficient computations on top of it, and tools for integrating C, C++ and Fortran code.
SciPy includes a very large collection of numerical, statistical, and optimization algorithms; pandas provides R-style Data Frame objects on top of NumPy data structures for fast computation, and matplotlib~\parencite{Hunter:2007} is a Python 2D plotting library, also used in Julia. 
\\
At the same time, Python has already a huge number of well-known libraries, also usable for data analysis. Beautiful Soup\footnote{http://www.crummy.com/software/BeautifulSoup/, 15.03.2015.} is for example a great library to analyze html or xml data. Together with the Python community and other libraries it makes Python a attractive tool for data analysis.


\section{Big Data Platforms}
The presented tools in the previous sections are great when working with megabytes and gigabytes of data, which is very common for data analysis: Often, data scientists spend much time on working on a small sample of a larger set, aggregated results, or simply on a small data set. However, as data size grows big data frameworks like Hadoop are receiving a lot of attention: They provide an affordable, reliable and ubiquitous way to spread computation over tens or hundreds of CPUs on commodity hardware. In this section we discuss the most important trends of the big data community for data mining, in particular Apache Hadoop~\parencite{hadoop}, Mahout~\parencite{mahout} and Spark~\parencite{spark}.
\\
Apache Hadoop is an open-source software for reliable, scalable, distributed storage and processing of large data sets across large clusters of computers. The heart of Hadoop is the Hadoop Distributed File System (HDFS) and Hadoop MapReduce, a simple programming model for distributed processing. 
\\
The Hadoop distributed file system (HDFS)~\parencite{hdfs} is a distributed, scalable, and portable file system written in Java for the Hadoop framework. It is highly fault-tolerant and provides high throughput access to application data. HDFS stores large files as blocks replicated across multiple machines.
\\
MapReduce is a programming model originally developed by Google~\parencite{mapreduce} for processing large amounts of data in parallel on large clusters of commodity hardware. A MapReduce job splits the input data into chunks processed by the map tasks in parallel. The output of the map tasks is then the input of the reduce tasks which merges the intermediate results. Hadoop MapReduce provides an open-source software framework for writing MapReduce jobs in a reliable, fault-tolerant manner. 
Several algorithms have been implemented on the MapReduce programming model, e.g. a parellel k-Means algorithm~\parencite{parallelkmeans}. 
\\
Numerous Apache Software Foundation projects are built on top of Hadoop, such as Hive~\parencite{hive} and Cassandra~\parencite{cassandra}. For data mining, Apache Mahout~\parencite{mahout} is certainly the most important project: It is a library of scalable machine learning and data mining algorithms, implemented on top of Apache Hadoop and using the MapReduce paradigm. 
For data stored on the Hadoop Distributed File System, Mahout provides the data science tools to automatically find meaningful patterns in those big data sets: The Apache Mahout project aims to make it faster and easier to get information out of large data sets.
Therefore, Mahout provides the implementation of various data mining algorithms for Hadoop, executable in local mode (if MapReduce is not applicable) or in distributed mode. 
\\
In 2014, the Mahout community decided to move its code base onto a more modern data processing system that offers a richer programming model and more efficient execution than Hadoop MapReduce: Apache Spark. Apache Spark is a data analytics cluster computing framework able to work on top of the Hadoop Distributed File System, but also with data from Hive, Cassandra or any other Hadoop input format. In contrast to Hadoop’s MapReduce, Spark's richer programming model leads to tremendous performance gains for some applications. Spark also provides in memory cluster computing, making it well-suited for data mining algorithms. 


\section{Middleware Tools}
While all of the presented environments are frequently used by data scientists, they demonstrate one decisive drawback: Before executing data mining algorithms, the data first has to be fetched from the database and transformed into a format readable by these tools. Therefore it is an enhancement to bring the algorithms closer to the database. In this section we present MADlib~\parencite{MADlib} and RapidMiner~\parencite{rapidminer}, two tools that try to fill this gap.
\\
MADlib is an open-source library providing a suite of SQL-based algorithms for machine learning, data mining and statistics, that can run within a database engine. Therefore, it is not necessary anymore to import or export data with an ETL process from the database to the respective data mining tool. The analytical algorithms of MADlib can be installed and executed within a relational data base engine as long the engine supports extensible SQL. So far, MADlib works with PostgresSQL\footnote{http://www.postgresql.org/, 15.03.2015.}, Pivotal Greenplum\footnote{http://pivotal.io/de/big-data/pivotal-greenplum-database, 15.03.2015.} and Pivotal HAWQ\footnote{http://pivotal.io/de/big-data/pivotal-hawq, 15.03.2015.}.
\\
MADlib's main algorithms are written in declarative SQL statements using extensible SQL; higher level tasks such as iterations and specific program structures are implemented in Python code. This code drives the algorithms and invokes data intensive computations directly in the database.
\\
RapidMiner is another tool for predictive analysis with a strong focus on visual development. It's main goals are low entry barriers and quick speed up. Therefore, it's main audience are not only data scientists and IT-specialists, but also the business department and stakeholders. To achieve this, RapidMiner let end users to design data analysis routines with a graphical user interface.
\\
RapidMiner provides three different analytical engines for data mining. By default, all algorithms and computations are executed in memory. Since random access is often necessary for data mining algorithms this is the fastest approach for medium size data sets. In contrast, in database mining brings the algorithms into the database, therefore the loading phase of the memory processing gets redundant. This solution is offered for different database systems utilizing database functionality. Finally, in Hadoop computation allows to combine the user interface of RapidMiner with the workflows of Hadoop clusters to analyze large data sets.
\\
Interestingly, the execution of algorithms in the database is not designed for performance, but for high data locality. RapidMiner's performance is the best when the data is first loaded and executed in the in memory engine. Data mining algorithms in HyPer can benefit from both: High data locality and high performance.


\section{Data Mining in Databases}
Software like MADlib and RapidMiner often combine SQL operators with high-level programming constructs. Since this is not ideal for the performance of the algorithm, database vendors such as Oracle and SAP are going one step further and providing their databases with more and more statistical and data mining functionalities using the existing SQL syntax and graphical user interfaces.
\\
SAP HANA~\parencite{SAP} is SAP's main memory database. Its functionality is very similar to HyPer's: It combines transactional processing (OLTP) and analytical processing (OLAP) into one system without the need of a database and an additional datawarehouse. 
SAP PAL~\parencite{pal} is an extension for HANA to provide various data mining algorithms. The idea is similar to HyPer's: Instead of importing and exporting data to other, external tools, the best place to perform data mining algorithms is directly in the database. Therefore, PAL's complex analytical computations can be performed very efficient and fast in memory. The transfer time of large tables from the database to the application becomes redundant and calculations are much more inexpensive.
\\
As one of the market leaders for traditional, disk-based databases, Oracle provides a lot of possibilities for data mining and knowledge discovery its databases, such as the Oracle Analytical SQL Features and Functions\footnote{http://www.oracle.com/technetwork/database/bi-datawarehousing/sql-analytics-index-1984365.html, 15.03.2015.} and the Oracle Statistical Functions\footnote{http://www.oracle.com/technetwork/middleware/bi-foundation/index-092760.html, 15.03.2015.}. The Oracle Analytical SQL Features and Functions are a suite to extend the already existing SQL syntax by a wider range of analytical features such as rankings, windowing and reporting aggregates. Oracle Statistical Functions enhance the normal functionality by statistics such as descriptive statistics, hypothesis testing, correlations analysis, cross tabs and the analysis of variance. 
\\
On top of this ranks the Oracle Advanced Analytics Options~\parencite{oracle}. It provides techniques for state-of-the-art data mining technologies by implementing in database algorithms in the SQL language and by integrating R algorithms. Oracle Data Mining (ODM) provides algorithms as native SQL for high performance. As previously, an importing or exporting of data to other tools becomes redundant. Users can either use the Oracle Data Miner\footnote{http://www.oracle.com/technetwork/database/options/advanced-analytics/odm/index.html, 15.03.2015.}, a work flow based GUI tool, or a SQL API. Additionally, Oracle R Enterprise (ORE)\footnote{http://www.oracle.com/technetwork/database/database-technologies/r/r-enterprise/overview/index.html, 15.03.2015.} integrates the already presented R language for statistical computing with the database. Own R packages and extensions can be implemented and executed in the database without additional data loading.
\\
\\
In this chapter we gave a broad overview over state-of-the-art data mining software, frameworks and languages. A subset of these tools will be used in the experiments in Chapter~\ref{chapter:evaluation} to compare their performance against the performance of the HyPer k-Means operator, which implementation will be described in the next chapter.

