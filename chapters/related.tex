\chapter{Related Systems}\label{chapter:related}

In this chapter we present state-of-the-art data mining frameworks and languages, their main characteristics and areas of application. We try to give a broad overview starting with research tools and dynamic languages, big data solutions, software systems that are strongly integrated with a database, and finally in database processing.

\section{Research Tools}
In this section we discuss two data mining tools - Weka~\parencite{Hall:2009:WDM:1656274.1656278} and ELKI~\parencite{DBLP:conf/ssdbm/AchtertKZ08} - that are strongly originated in the research community and developed at Universities. Additionally, Weka is often used for teaching data mining techniques. Nevertheless, both tools, in particular the more mature Weka are used in practice by industrial scientists too.
\\
Weka is a data mining software~\enquote{workbench} developed at the University of Waikato that gained big attentation in the machine learning and data mining community over the past decade. Weka provides and integrates a large collection of algorithms for all aspects of data mining such as data preprocessing, classification, regression, clustering, association rule mining and visualization into one framework. Weka is written in Java and provides a stand-alone GUI with the~\enquote{Weka Explorer}. Weka programs can also be executed directly within Java programs. 
\\
Weka's default file format are arff files, which is an ASCII text file that describes instances and its attributes. In contrast to csv files, arff files can be read incrementally by Weka, because the header of arff files already determines whether a column is numeric or nominal. In contrast, all instances have to be inspected when using csv files. Therefore, arff files provide a performance improvement and provide high accuracy about the data type of attributes.
\\
ELKI (Environment for Developing KDD-Applications Supported by Index Structures) is a relatively new data mining framework developed at the Ludwigs-Maximilians-Universität München in 2009 with an even stronger focus on research. It's main objective is to enable a fair comparison of data mining algorithms based on experimental evaluation. In a strongly evolving field as data mining, many algorithms are released every year and are never be used again as the paper is published. Often, the comparison of algorithms is based on a experimental evaluation, which is not always open and reproducible. Secondly, the code is not always published, well-documented and clear to use. ELKI provides an implementation framework for new data mining algorithms, leading to a better comparability among them and therefore to a fairer evaluation of the newly proposed algorithm. 
\\
As Weka, ELKI is written in Java and can be either used via graphical user interface or within Java programs.A major strengths of the framework is that it is able to read arbitrary data types and the support of any distance or similarity measure. Therefore, all kinds of complex data types can be integrated into existing algorithms, only by providing a distance function for the given data type. ELKI also encourages the use of index-structures to achieve performance gains when working with high-dimensional data sets. 


\section{Dynamic Languages for Statistical Computing}

While Weka and ELKI both provide an excellent suite for data mining, data scientists often prefere a more dynamic language over bulky Java code. In particular for exploratory data analysis, dynamic scripting languages are very popular to get a first feeling for the given data. Special purpose languages exist, with a syntax and built-in data structures that make common data analysis tasks both faster and more concise. This section presents the R~\parencite{} language which is popular for decades, the previously announced Julia~\parencite{} language and the Python ecosystem SciPy~\parencite{}. R and Julia are both special-purpose languages designed for data analysis. While Python is a general-purpose language, the SciPy ecosystem provides excellent tools for data analysis and data mining with similar syntax and data structures.
\\
R is a language for statistical computing and data analysis with a rich set of graphical techniques makes it often researcher's number one tool for creating graphics for publications. It is designed as a true computer language, and most of the R libraries are written in R itself, however, C, C++ and Fortran code can be linked and called at run time and is often used for computationally-intensive tasks. It is also possible to manipulate R objects via C code directly.
\\
R sees itself more as an environment for statisitcal techniques than just a programming languages. It can be extended via packages, and for most algorithms plenty of packages are available via the CRAN network. RStudio provides an IDE for R programmers, and software products like JGR and R Commander provide a GUI for R programs.
\\
Thanks to the underlying C, C++ and Fortran libraries, R can do matrix math very fast. However, the R language interpreters is rather slow, which discourages writing large libraries or complex abstractions in the R languages themselves. Instead the use of C, C++ or Fortran extensions very important.
\\
Julia is a modern dynamic programming language for scientific computing, designed to address some of these concerns. As R, Julia is a programming language itself written mainly in Julia, using C and Fortran for computationally expensive tasks. In contrast to R, Julia uses an LLVM-based just-in-time (JIT) compilation for interpreting its own language, and therefore is often matched with the performance of C. The same compilation technique is used in HyPer, therefore it will be interesting to compare both techniques.
\\
This results in stunning performance results. While the running time for matrix multiplications does not change, since also R is using C++, basic language features differ tremendeslouy.
For data analysis, external packages are available allowing the execution of state-of-the-art data mining algorithms.
Julia’s weakness, however, is its libraries. R has CRAN, certainly the most impressive collection of statistical libraries available anywhere. Julia also lacks a rich development environment, like RStudio, and has only rudimentary support for plotting, which is a pretty critical part of most exploratory data analysis. Julia does, however, have a very active community
\\
SciPy is a Python environment enhancing the Python language into a data analysis environment. The NumPy library provides a solid matrix data structure, with efficient matrix and vector operations. 
SciPy includes a very large collection of numerical, statistical, and optimization algorithms.
Pandas provides R-style Data Frame objects (using NumPy arrays underneath to ensure fast computation), along with a wealth of tools for manipulating them.
\\
At the same time, Python has a huge number of well-known libraries for the messier parts of analysis. For example, Beautiful Soup is best-of-breed for quickly scraping and parsing real-world HTML. Together with Python’s strong community and other libraries it makes Python a compelling alternative


\section{Big Data Platforms}
The presented tools in the previous section are great when working with megabytes and gigabytes of data, which is very common. Often, data scientists spend much time on working on a small sample of a larger set, an aggregated results, or the data set is not that big at all.
\\
However, as data size grows big data frameworks like Hadoop are receiving a lot of attention: It provides an affordable, reliable and ubiquitous way to spread computation over tens or hundreds of CPUs on commodity hardware. In this section we discuss the most important trends of the big data community for data mining. Apache Hadoop is an open-source software for reliable, scalable, distributed storage and processing of large datasets across large clusters of computers. The heart of Hadoop is the Hadoop Distributed File System (HDFS) and Hadoop MapReduce, a simple programming model for distributed processing. 
\\
The Hadoop distributed file system (HDFS) is a distributed, scalable, and portable file-system written in Java for the Hadoop framework. It is highly fault-tolerant and provides high throughput access to application data. HDFS stores large files as blocks replicated across multiple machines.
\\
MapReduce is a programming model originally developed by Google~\parencite{mapreduce} for processing large amounts of data in parallel on large clusters of commodity hardware. A MapReduce job splits the input data into chunks processed by the map tasks in parllel. The output of the map tasks are then the input of the reduce tasks which merges the intermediate results. Hadoop MapReduce provides an open-source software framework for writing MapReduce jobs in a reliable, fault-tolerant manner. 
Several Algorithms can be performed on the MapReduce programming model, e.g. k-Means~\parencite{parallelkmeans}. For our research, it will be interesting to see how HyPer works with Big Data compared to Apache Hadoop.
\\
Numerous Apache Software Foundation projects are based on top of Hadoop, such as Hive~\parencite{hive} and~\parencite{cassandra}. Apache Mahout is a library of scalable machine-learning algorithms, implemented on top of Apache Hadoop and using the MapReduce paradigm. 
Once big data is stored on the Hadoop Distributed File System (HDFS), Mahout provides the data science tools to automatically find meaningful patterns in those big data sets. The Apache Mahout project aims to make it faster and easier to turn big data into big information.
Mahout provides an implementation of various machine learning algorithms, some in local mode and some in distributed mode (for use with Hadoop). Each algorithm in the Mahout library can be invoked using the Mahout command line.
\\
In 2014, the Mahout community decided to move its codebase onto modern data processing systems that offer a richer programming model and more efficient execution than Hadoop MapReduce: Apache Spark. Apache Spark is a data analytics cluster computing framework, working on top of the Hadoop Distributed File System (HDFS). In contrast to Hadoop’s MapReduce, Spark comes with a richer programming model, leading to tremendous performance gains for some applications. Spark also provides in-memory cluster computing, making it well-suited to data mining algorithms. 
\\
Spark is a fast and general processing engine compatible with Hadoop data. It can run in Hadoop clusters or Spark's standalone mode, and The intention is to enhance the Hadoop Stack.it can process data in HDFS, Cassandra, Hive, and any Hadoop InputFormat. It is designed to perform both batch processing (similar to MapReduce) and new workloads like streaming, interactive queries, and machine learning.


\section{Middleware Tools}
While all of the presented environments are frequently used by data scientists, they demonstrate one decisive drawback: Before executing data mining algorithms, the data first has to be fetched from the database and transformed into a format readable by these tools. Therefore a first enhancement is to bring the algorithms closer to the database. In this section we present MADlib and RapidMiner.
\\
MADlib is an open-source library providing a suite of SQL-based algorithms for machine learning, data mining and statistics, that can run with scale within a database engine. Therefore, it is not need to import/export data with an ETL process to other tools. The analytic methods can be installed and executed within a relational data base engine as long the engine supports extensible SQL. So far, MADLib works with Postgres, Pivotal Greenplum and Pivotal HAWQ.
\\
MADlib's main functionality is written in declarative SQL statements which organize the date movement to and from disk on the current or on network machines. Loops running on a single CPU can benefit from SQL extensibility and call high performance math libraries in user defined functions. Higher level tasks can be implemented in Python code, that drives the algorithms and invokes data-rich computations that are computed in the database.
\\
RapidMiner is another tool for predictive analysis with a strong focus on visual development. It's main goals are low entry barriers and quick speed up. It's main audience is not only Data Scientists and IT Specialists, but also the business department and stakeholders. Therefore RapidMiner sees itself as collaboration tool. The main feature for end users of RapidMiner is the graphical user interface to  design data analysis routines. Therefore data mining queries can be created with a few clicks only.
\\
RapidMiner provides three different analytic engines to deal with different data volumes, data variety and velocity of data. In general, all analytical algorithms and computations are in-memory. Since random access is often necessary for data mining algorithms this is often the fastest approach for medium size data sets. In-database mining brings the algorithms into the database, therefore the loading phase to get the data is abolished. This solution is offered for different database systems utilizing database functionality. And finally, in-hadoop compuations allows to combine the user interface of RapidMiner with the workflows of Hadoop Clusters. Therefore terabytes and petabytes of data can be analyzed with RapidMiner.
\\ 
For our use case, the in-database processing of RapidMiner seems the closest related to our approach. However, the in-database engine is designed for performance, but for high data locality. That means that no ETL process is necessary if algorithms are executed on the database. However, RapidMiner's performance is best if the data is first loaded and executed in the in-memory engine.


\section{Knowledge Discovery in Databases}
However, for complex algorithms SQL operators have to be combined with high-level constructs, often resulting in bad performance. Therefore, database vendors such as Oracle and SAP are going one step further and providing their databases with more and more statistical and data mining functionalities using the existing SQL syntax and graphical user interfaces.
\\
SAP HANA Predictive Analysis Library (PAL): SAP HANA is SAP's main memory database. Its functionality is very similar to the one that HyPer proves: It combines transactional processing (OLTP) and analytical processing (OLAP) on one system. Therefore that database and the datawarehouse are combined into one database. 
\\
SAP PAL~\parencite{pal} is an extension for HANA to implement data mining algorithms in the areas of association, clustering and classification. The idea is similar to HyPer's: Instead of importing/exporting data to other, external tools, the best place to perform data mining algorithms is right on the database. Therefore, PAL's complex analytic computations are performed directly on the database with very high performance. The transfer time of large tables from the database to the application becomes redundant and calculations are much more inexpensive.
\\
Oracle: As one of the market leaders for traditional, disk-based databases, Oracle provides a lot of possibilites for data mining and knowledge discovery on their database. By default, Oracle provides its databases with the Oracle Analytical SQL Features and Functions and the Oracle Statistical Functions. Oracle Analytical SQL Features are a suite to improve the already existing SQL syntax by wider range of analytical features such as rankings, windowing and reporting aggregates. Oracle Statistical Functions enhance the normal toolset by statistics such as descriptive statistics, hypothesis testing, correlations analysis, cross tabs and the analysis of variance (ANOVA). 
\\
On top of this rank the Oracle Advanced Analytics Options. It provides techniques for state-of-the-art data mining technologies by implementing in-database algorithms and R algorithms, accessible via SQL and the R language. Oracle Data Mining (ODM) provides datamining algorithms directly on the the database for improvemnts in performance. Also, a importing/exporting of data to other tools becomes redundant. Users can either use the Oracle Data Miner, a work flow based GUI, or the SQL API. Oracle R Enterprise (ORE) integrates the already presented R language for statistical computing and graphics with the database. Therefore, R algorithms can be executed directly on the database without an ETL process. Base R and popular R packages can be executed in-database, while every R package can be executed with embedded R while the database manages the data loading. This allows data scientists to write their own R packages and extension and bring the code to the database.
\\
\\
In this chapter we provided a broad overview over data mining tools and languages in general. Some of those tools will be used in Chapter~\ref{evaluation} to compare our k-Means HyPer implementations with existing tools.

