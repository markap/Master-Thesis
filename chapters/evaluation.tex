\chapter{Evaluation}\label{chapter:evaluation}

In this chapter we provide an extensive experimental evaluation of the HyPer k-Means operator and compare the different HyPer implementations to each other as well as to some of the state-of-the-art clustering tools presented in Chapter~\ref{chapter:related}. All experiments have been performed on a workstation equipped with sixteen 2.93 GHz CPUs and 64 MB of main memory.


\section{Data Sets}
As objective comparison criterion the execution time per iteration is measured in the following experiments, as described in Section~\ref{section:constraint}. Therefore the different technologies can be compared in a fair way and random initialization and internal improvements for faster convergence do not affect the outcome. For the experiments a real world data set is used and four synthetic data sets have been generated\footnote{The data set generation script can be found in Appendix~\ref{appendix:generator}.}. The synthetic data sets are chosen in a way that both the number of instances and the number of dimensions are varying, covering many real world use cases and also allow a reasonable comparison between the data sets. The used data sets are shown in~\autoref{tab:dataset}.

\begin{table}[htsb]
  \caption[The Data Sets]{The Data Sets.}\label{tab:dataset}
  \centering
  \begin{tabular}{l l l l l}
    \toprule
      & Instances & Dimensions & Size (byte) & Size (Gigabyte) \\
    \midrule
      3D Network        & 434,874    & 4     & 20,673,913 & 0.019 \\
      High Dimensional  & 50,000     & 50    & 10,000,000 & 0.009 \\
      Medium Size       & 15M       & 4     & 240,000,000 & 0.22 \\
      Medium Size HD    & 15M       & 50    & 3,000,000,000 & 2.79 \\
      Large Size        & 150M      & 10    & 6,000,000,000 & 5.59 \\
    \bottomrule
  \end{tabular}
\end{table}



\begin{itemize} 
\item 3D Network: The first data set contains the 3D spatial road network data of a road network in North Jutland, Denmark~\parencite{3dnet}. It is a real world data set available at the UCI Machine Learning Repository~\parencite{frank2010uci} and consists of 434,874 data points in four dimensions: The id of the road segment, the latitude, the longitude and the altitude of the segment. The id is a large integer, the other dimensions are floating point numbers. The size of the data set is 19.72 MB, therefore it is a rather small data set.

\item High Dimensional: As second data set a synthetic data set with 50,000 data points has been generated. The data set consists of 50 dimensions and is used to resemble a rather small but high dimensional data set. All dimensions are floating point numbers. The size of the data set is 9.50 MB, hence it is even smaller then the network data set.

\item Medium Size: Next, a larger data set has been created to test the tools on millions of tuples: The medium size data set consists of 15 million data points in four dimensions. The size of the data set is 228.88 MB, therefore almost 12 times larger than the network data set.

\item Medium Size High Dimensional (HD): For comparing the previous data set with a high dimensional data set, the medium size high dimensional data set has been generated. As the medium size data set, it consists of 15 million data points, this time in 50 dimension. This leads to a growth from 0.23 GB to 2.79 GB. The same number of instances with different dimensions allows us to evaluate how different algorithms behave when dimensions are growing.

\item Large Size: Finally, we generated an even larger data set. The number of instances has increased by a factor of 10 compared to the medium data set, resulting in 150 million instances. As number of dimensions, 10 has been chosen as a middle way between the previously chosen 4 and 50 dimensions. This leads to a data set of 5.59 GB.
\end{itemize} 

Additionally to varying the data sets, we also vary the cluster number $k$ from 3, 10 and 20 for each data set for each used tool. Since none of the data sets has a natural optimal clustering, there is no perfect $k$, therefore a varying $k$ works best for comparing different tools.


\section{Competitor Systems}

As already described in Chapter \ref{chapter:related}, there exist many software solutions and programming languages for data mining. In this section we look at a subset of the tools implementing the k-Means clustering algorithm and use them for comparison with our own k-Means HyPer operator. To make the results comparable in a fair manner, we only use tools that give enough information about the clustering process, e.g. the number of iterations, the cluster compactness and most importantly, the applied algorithm. Not all tools are implementing the Lloyd algorithm or, as for R, the default algorithm is the Hartigan and Wong algorithm, and the usage of the Lloyd algorithm has to be explicitly specified as an input parameter.
\\
Since k-Means is a non-deterministic algorithm, the number of iterations is the most important criterion to make the results comparable. The running time of k-Means depends almost entirely on the number of iterations the algorithm has to make, which is largely influenced by the non-deterministic initialization. Therefore the running time should be relative to the number of iterations. Unfortunately, neither ELKI's nor SciPy's k-Means implementation provide the number of iterations as output. Therefore, a fair comparison is not possible. Fortunately, Weka, R and Julia provide good configuration possibilities as well as a result set that contains information about the number of iterations. While Weka is written in Java, R and Julia are both written in their own high-level syntax. Even though, critical code parts are implemented in C, C++ and Fortran for better performance. Without the overhead of an entire database system behind, it will be interesting to compare HyPer with Weka, R and Julia.
\\
We are excluding the big data solutions, middleware tools and analytical databases presented in Chapter~\ref{chapter:related}, since these tools are often only applicable for specific use cases, e.g. tremendously large volumes of data, or require specific environments or proprietary software. Instead, we are comparing the HyPer k-Means solution with well-known, open-source clustering tools used by data scientists everyday.
\\
The usability and user experience among R and Julia is very similar: Both can be run as interactive program or as script in their functional language, and both can import the test data sets in csv format. HyPer can be run as interactive console program or as script, too, and provides efficient csv loading~\parencite{hypercsv}. Weka provides a GUI with the Weka Explorer, and can be run from the command line, too, or directly in Java programs. Internally, Weka uses the arff format for its algorithms. Using the Weka GUI to convert the original data (in our case in csv format) into the arff format feels fairly natural, as it happens in a preprocessing step when loading the data. However, when using the command line or Java programs to read a csv file, the file has to be preprocessed to the arff format first by an additional command. This takes time and is not as convenient compared to the other tools.
\\
However, the main reason for not including Weka in the next sections is the disappointing running time compared to Julia, R and HyPer, as depicted in~\autoref{tab:weka_final}. The table shows the median, the 90th and the 95th percentile of the running time per iteration of 100 Weka clusterings for $k = 3, 10$ and $20$. Compared to the other tools, which running time we will present in later sections in detail, Weka performs poorly: On the network data set, Weka is still slower by a factor of 7 compared to Julia's implementation, which is already the slowest tool on that data set. On the high dimensional data set, the HyPer C++ implementation shows the worst results, however, Weka is slower by a factor of around 6. As last test, the medium size data set was run against Weka, and it is slower by a factor of 10 for $k = 3$, 8 for $k = 10$ and 7 for $k = 20$ compared to Julia, the slowest one regarding this data set. Because of this tremendous differences in running time, we stopped the experiments with Weka and decided not to include Weka in the following sections and only compare the performance of the HyPer operator to R and Julia.

\begin{table}[htsb]
  \caption[Weka Results - Time per Iteration]{Weka Results - Time per Iteration in seconds.}
  \label{tab:weka_final}
  \centering
  \begin{tabular}{ll l l l |l l l |l l l }
    \toprule
      &  & \multicolumn{3}{c}{Network 3D} & \multicolumn{3}{c}{High Dimensional} & \multicolumn{3}{c}{Medium Size}  \\
      & \emph{k} & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 \\
    \midrule
      \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{percentile}}}} & 50  & 1.39 & 1.54 & 1.93 & 1.08 & 1.39 & 1.90 & 54.01 & 58.25 & 71.11 \\
      & 90  & 1.48 & 1.60 & 2.02 & 1.16 & 1.44 & 1.98 &59.16 & 62.54 & 87.16 \\
      & 95  & 1.49 & 1.61 & 2.06 & 1.19 & 1.46 & 2.01 & 59.79 & 64.31 & 88.54 \\
    \bottomrule
  \end{tabular}
\end{table}



\section{Serial Implementation}\label{section:serial}

In this section we compare the C++ driven and the LLVM driven implementation, both described in detail in Section~\ref{section:serial_implementation}. The experiments differentiate between the compilation and the execution time. The compilation time is the time for generating LLVM code, while the execution time is the time for executing the k-Means algorithm after the LLVM code was generated. Since the compilation time can be seen as a one-time setup time, we expect the execution time to be much larger. 
\\
For each data set, the two HyPer implementations are tested for $k = 3,10$ and 20. For each $k$ the algorithm is executed 100 times with a maximum number of iterations of 10. As result, the median time per iteration in seconds is stated. For better comparability the compilation time is also relative to the number of iterations, even though it stays constant as the number of iterations grows.
\\
~\autoref{tab:network_serial} shows the result of the two serial implementations for the network data set. As we observe the compilation time is independent of the cluster number $k$. Also the compilation time does not differ among the C++ implementation and the LLVM version. In contrast, the execution time increases as the cluster number $k$ increases. This is true for both implementations. 

\begin{table}[htsb]
  \caption[3D Network - Time per Iteration]{3D Network - Time per Iteration.}\label{tab:network_serial}
  \centering
  \begin{tabular}{l l l l l}
    \toprule
      & HyPer C++ & & HyPer LLVM & \\
      k & compilation[s] & execution[s] & compilation[s] & execution[s] \\
    \midrule
      3 & 0.0050 & 0.0680 & 0.0046 & 0.0171 \\
      10 & 0.0044 & 0.0947 & 0.0046 & 0.0502 \\
      20 & 0.0045 & 0.1391 & 0.0046 & 0.0901 \\
    \bottomrule
  \end{tabular}
\end{table}


For better visualization, ~\autoref{fig:hyper_network} depicts the same result as stacked bar charts. Again we observe that the compilation time is low and constant, while the execution time differs among the implementations. For $k = 3$, the execution time of the C++ version is almost four times the execution time of the LLVM version. For $k = 10$ it is two times the execution time and for $k = 20$ it is 1.5 times, respectively. On the other hand that means that for a larger cluster number $k$, the LLVM execution time grows  faster than the execution time of the C++ version. Actually, from $k = 3$ to $k = 10$, the LLVM version grows by a factor of 2.9, while the C++ version grows by 1.4. From $k = 3$ to $k = 20$ the growth is even more significant, from factor 2 for the C++ version to 5.3 for the LLVM version.


\begin{figure}[htsb]
  \centering
  \includegraphics[scale=0.5, trim="0cm 1.5cm 0cm 0cm"]{figures/charts/hyper_network}
  \caption[3D Network - Time per Iteration]{3D Network - Time per Iteration.}
  \label{fig:hyper_network}
\end{figure}

This gives us the first interesting results. Although the C++ version is implementing k-Means using LLVM only for generated functions, the compilation time does not differ compared to the LLVM version, where not only the functions but also the entire algorithm is written in LLVM. Nevertheless, the functions for computing the distance and updating the centers are generated in LLVM for both implementations which is an explanation for the similar compilation times: In the C++ version this code is generated as functions callable from the \texttt{runtime system}, while the LLVM version embeds the code directly into the LLVM program structure. Therefore, the difference between the two seems to be insignificant.
\\
Regarding the execution time, the LLVM version is much faster than the C++ version. One reason is that the C++ implementation has many function calls between the \texttt{compile time} and the \texttt{runtime system}. For the LLVM system these calls are not necessary, therefore the data can remain in the CPU registers. The second advantage is that the entire algorithm is compiled in LLVM code resulting in very efficient code, optimized on a lower level compared to the C++ code.


\begin{table}[htsb]
  \caption[High Dimensional - Time per Iteration]{High Dimensional - Time per Iteration.}\label{tab:hd_serial}
  \centering
  \begin{tabular}{l l l l l}
    \toprule
      & HyPer C++ & & HyPer LLVM & \\
      k & compilation[s] & execution[s] & compilation[s] & execution[s] \\
    \midrule
      3  & 0.1278 & 0.0522 & 0.0933 & 0.0327 \\
      10 & 0.1287 & 0.1171 & 0.0933 & 0.0706 \\
      20 & 0.1299 & 0.2070 & 0.0933 & 0.1252 \\
    \bottomrule
  \end{tabular}
\end{table}

~\autoref{tab:hd_serial} shows the result of the same experiment with the high dimensional data set. In contrary to the network data set the compilation time is slightly different between the C++ and the LLVM implementation. Furthermore for both versions the compilation time is larger than the execution time for $k = 3$ and almost equal for $k = 10$. Only for $k = 20$ the execution time is larger than the compilation time, as depicted in~\autoref{fig:hyper_50000}.

\begin{figure}[htsb]
  \centering
  \includegraphics[scale=0.5, trim="0cm 1.5cm 0cm 0cm"]{figures/charts/hyper_50000}
  \caption[High Dimensional - Time per Iteration]{High Dimensional - Time per Iteration.}
  \label{fig:hyper_50000}
\end{figure}

Again, the compilation time stays constant for different values of $k$. The increase of compilation time compared to the previous network data set is induced by the high dimensionality: For each dimension additional code has to be generated. Therefore the code for a data set with four dimension is faster to compile than for a data set with 50 dimensions. 
The difference in the running time is similar to the network data set even though the increase in performance between the two implementations is not as significant. For $k = 3$, LLVM is faster by a factor 1.6 and for $k = 10$ and $k = 20$ by 1.7. This time the increase in execution time is strongly correlated. From $k = 3$ to $k = 10$, the LLVM version and the C++ version grow both by a factor of 2.2. From $k = 3$ to $k = 20$, the LLVM version grows by a factor of 4, while the C++ version grows by 3.8.
\\
Interestingly, the execution time for the network data set is lower than for the high dimensional data set, even though the network set has a size of 0.019 GB, while the high dimensional data set has only 0.009 GB. Even if we omit the compilation time which stays constant for a growing number of iterations and only look at the execution time the network data set is only for the C++ implementation and $k = 3$ slower than the high dimensional data set. Apparently, our implementation performs much better for low dimensional data sets, where not that much code has to be generated. As already mentioned, for each additional dimension additional code has to be generated and executed. For small data sets this compilation time can be a limiting factor, in addition to the extended execution time for high dimensional data sets in general.
\\
~\autoref{tab:med_serial} and~\autoref{tab:med_hd_serial} show the same experiment for the medium size and the medium size high dimensional data set. Both consist of 15 million instances, the first of four dimensions and the second of 50 dimensions. The compilation time of the medium size data set is similar to the network data set since both consist of four dimensions. The same is true for the medium size high dimensional data set and the previously used high dimensional data set: Both consist of 50 dimensions and the compilation time is quite similar. This shows us that the compilation time is independent of the data size and only affected by the number of dimensions. However, as the number of instances grows, the compilation time is not a significant factor for the overall running time anymore, as~\autoref{fig:hyper_15Mxhd} shows: The compilation time is not even visible plotting the data as stacked bar charts. 
\\
Again, the LLVM version shows a much better performance compared to the C++ version. For the medium size data set the overall running time is faster by a factor of 2.5, 1.6 and 1.4 for $k = 3, 10$ and 20, and for the medium size high dimensional data set by a factor of 1.6, 1.5 and 1.5, respectively.

\begin{table}[htsb]
  \caption[Medium Size - Time per Iteration]{Medium Size - Time per Iteration.}
  \label{tab:med_serial}
  \centering
  \begin{tabular}{l l l l l}
    \toprule
      & HyPer C++ & & HyPer LLVM & \\
      k & compilation[s] & execution[s] & compilation[s] & execution[s] \\
    \midrule
      3  & 0.0041 & 2.3779 & 0.0047 & 0.9191 \\
      10 & 0.0041 & 3.2856 & 0.0047 & 2.1001 \\
      20 & 0.0041 & 4.6765 & 0.0047 & 3.4518 \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}[htsb]
  \caption[Medium Size High Dimensional - Time per Iteration]{Medium Size High Dimensional - Time per Iteration.}
  \label{tab:med_hd_serial}
  \centering
  \begin{tabular}{l l l l l}
    \toprule
      & HyPer C++ & & HyPer LLVM & \\
      k & compilation[s] & execution[s] & compilation[s] & execution[s] \\
    \midrule
      3  & 0.1127 & 16.2787 & 0.0935 & 10.3302 \\
      10 & 0.1127 & 34.0051 & 0.0935 & 22.0209 \\
      20 & 0.1126 & 59.2904 & 0.0935 & 38.7046 \\
    \bottomrule
  \end{tabular}
\end{table}

Obviously the execution time for the high dimensional data set is much larger compared to the low dimensional data set: This time the high dimensional data set has the same number of instances but 50 dimensions. For the C++ implementation the execution time is increased by a factor of 6.8 for $k = 3$, 10.3 for $k = 10$ and 12.6 for $k = 20$. For the LLVM implementation it is 11.2, 10.5, 11.2, respectively. Since the high dimensional data (2.79 GB) set is larger than the medium size data set (0.22 GB) by a factor of 12.7, the values are strongly correlated to the growth of the data set.

\begin{figure}[htsb]
  \centering
  \includegraphics[scale=0.5, trim="0cm 1.5cm 0cm 0cm"]{figures/charts/hyper_15Mxhd}
  \caption[Medium Size High Dimensional - Time per Iteration]{Medium Size High Dimensional - Time per Iteration.}
  \label{fig:hyper_15Mxhd}
\end{figure}


Finally,~\autoref{tab:large_serial} shows the same experiment for the large size data set. The data set consists of ten dimensions and therefore the compilation time increases compared to the data sets with four dimensions by a factor of around two. However, since the data set has a size of 5.59 GB we can omit the compilation time since it does not affect the overall running time. Comparing the two versions regarding the overall running time, we observe that the LLVM version provides again much better results and is faster by a factor of 2.0, 1.5 and 1.4 for $k = 3, 10$ and 20 compared to the C++ implementation. This results in a difference of around 20 seconds per iteration between the two implementations.



\begin{table}[htsb]
  \caption[Large Size - Time per Iteration]{Large Size - Time per Iteration.}
  \label{tab:large_serial}
  \centering
  \begin{tabular}{l l l l l}
    \toprule
      & HyPer C++ & & HyPer LLVM & \\
      k & compilation[s] & execution[s] & compilation[s] & execution[s] \\
    \midrule
      3  & 0.0088 & 37.2804 & 0.0097 & 18.3734 \\
      10 & 0.0088 & 62.0034 & 0.0097 & 40.4783 \\
      20 & 0.0191 & 92.5868 & 0.0097 & 67.2364 \\
    \bottomrule
  \end{tabular}
\end{table}


In conclusion, we observed that the compilation time is not affected by the number of instances but the number of dimensions, where high dimensional data sets slow down the compilation phase. Interestingly, there is no significant difference regarding the compilation time between the two HyPer implementations. 
\\
Usually, the execution time outnumbers the compilation time by several factors. The only exception is the small, high dimensional data set as shown in the experiment. Here, the compilation time is slower than the execution time for small $k$. Furthermore the execution time grows by the number of instances and is much faster for the LLVM implementation. With equality regarding the compilation and a performance increase regarding the execution time the LLVM version was the best choice for all performed experiments and the implementation aspects should be used as building block for future HyPer operators for data mining.


\section{Performance Comparison with Competitor Systems}\label{section:performance}

In this section we compare our serial HyPer implementations with R and Julia’s implementation of the k-Means algorithm. As algorithm, the Lloyd algorithm with a maximum iteration number of ten is chosen. Apart from the large data set, the algorithm is executed 100 times for $k = 3, 10$ and $20$, respectively. For the large data set, the algorithm is executed ten times. The result is then presented as the execution time per iteration. For each algorithm the median, the 90th percentile and the 95th percentile are stated for each $k$. 

\begin{table}[htsb]
  \caption[3D Network - Time per Iteration]{3D Network - Time per Iteration in seconds.}
  \label{tab:network_all1}
  \centering
  \begin{tabular}{ll l l l |l l l |l l l |l l l}
    \toprule
      & & \multicolumn{3}{c}{Julia} & \multicolumn{3}{c}{R} & \multicolumn{3}{c}{HyPer C++} & \multicolumn{3}{c}{HyPer LLVM}  \\
      & \emph{k} & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 \\
    \midrule
      \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{percentile}}}} & 50  & 0.21 & 0.22 & 0.29 & 0.03 & 0.04 & 0.08 & 0.08 & 0.10 & 0.14 & 0.02 & 0.06 & 0.10 \\
      & 90  & 0.27 & 0.30 & 0.32 & 0.06 & 0.06 & 0.10 & 0.09 & 0.12 & 0.20 & 0.03 & 0.06 & 0.10 \\
      & 95  & 0.31 & 0.35 & 0.35 & 0.08 & 0.07 & 0.11 & 0.10 & 0.13 & 0.22 & 0.03 & 0.06 & 0.10 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[htsb]
  \centerline{
  \includegraphics[scale=0.4, trim="0cm 1cm 0cm 0cm"]{figures/charts/network_all}
  }
  \caption[3D Network - Time per Iteration]{3D Network - Time per Iteration.}
  \label{fig:network_all}
\end{figure}

First, we test against the real world network data set. The result is presented in~\autoref{tab:network_all1}. As we already know, the HyPer LLVM implementation outnumbers the C++ version. For the median, the LLVM implementation is 3.4, 1.8 and 1.5 times faster for $k = 3, 10$ and $20$. Julia performs the slowest, our LLVM implementation is 9.5, 4.0 and 3.1 times faster, respectively. Only the R implementation can compete with the HyPer LLVM operator and is with 0.7, 0.8 and 0.8 for $k = 3, 10$ and 20 even a bit faster. However, all tested programs differ in a few hundred milliseconds therefore the differences are not very significant.
\\
~\autoref{fig:network_all} depicts the result as a boxplot. We observe that the C++ version, the LLVM version and R are closely together, while Julia lays behind. Also regarding the variance of the 100 runs, Julia shows the strongest variations and many outliers.

\begin{table}[htsb]
  \caption[High Dimensional - Time per Iteration]{High Dimensional - Time per Iteration in seconds.}
  \label{tab:highdim_all}
  \centering
  \begin{tabular}{l l l ll |l l l |l l l |l l l}
    \toprule
      && \multicolumn{3}{c}{Julia} & \multicolumn{3}{c}{R} & \multicolumn{3}{c}{HyPer C++} & \multicolumn{3}{c}{HyPer LLVM}  \\
      &\emph{k} & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 \\
    \midrule
      \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{percentile}}}} &50  & 0.04 & 0.05 & 0.07 & 0.03 & 0.05 & 0.08 & 0.18 & 0.24 & 0.35 & 0.13 & 0.16 & 0.22 \\
      &90  & 0.04 & 0.05 & 0.07 & 0.04 & 0.06 & 0.10 & 0.21 & 0.34 & 0.40 & 0.13 & 0.16 & 0.22 \\
      &95  & 0.04 & 0.05 & 0.07 & 0.05 & 0.07 & 0.11 & 0.22 & 0.35 & 0.44 & 0.13 & 0.17 & 0.22 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[htsb]
  \centerline{
  \includegraphics[scale=0.4, trim="0cm 1cm 0cm 0cm"]{figures/charts/50000_all}
  }
  \caption[High Dimensional - Time per Iteration]{High Dimensional - Time per Iteration.}
  \label{fig:50000_all}
\end{figure}



Performing the same experiment on the high dimensional data set we see a different result, as depicted in~\autoref{tab:highdim_all}. As already shown, the HyPer C++ and LLVM versions are slower compared to the network data set even though the data set is smaller by size. R takes almost the same time for both data sets. Interestingly, Julia’s k-Means algorithm is now as fast as the R implementation, for $k = 10$ and 20 it is even faster. This is also depicted by the boxplot in~\autoref{fig:50000_all}. Also the variance changed: The HyPer C++ version shows the highest variance with many outliers, while Julia's variance is very small this time.   
\\
This result gives us interesting knowledge about the different implementations. Both HyPer operators perform poorly when dimensions increase. An increase in dimensions and a decrease in instances does not affect R significantly. On the other hand, Julia shows much better results as dimensions increase and seems to be optimized for high dimensions: It is 5.4, 4.4 and 4.2 times faster for $k = 3, 10$ and 20 on the high dimensional set compared to the network data set, and the variance is much lower. A conversation with Prof. Timothy E. Holy, Ph.D. substantiated this hypothesis\footnote{Julia User Group:~\url{http://bit.ly/1anXGMF}, 25.01.2015.}. 

\begin{table}[htsb]
  \caption[Medium Size - Time per Iteration]{Medium Size - Time per Iteration in seconds.}
  \label{tab:medium_all}
  \centering
  \begin{tabular}{l l l ll |l l l |l l l |l l l}
    \toprule
      && \multicolumn{3}{c}{Julia} & \multicolumn{3}{c}{R} & \multicolumn{3}{c}{HyPer C++} & \multicolumn{3}{c}{HyPer LLVM}  \\
      &\emph{k} & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 \\
    \midrule
      \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{percentile}}}} &50  & 5.42 & 7.61 & 10.62 & 0.77 & 1.61 & 2.50 & 2.38 & 3.29 & 4.68 & 0.92 & 2.10 & 3.46 \\
      &90  & 5.43 & 7.65 & 10.71 & 0.79 & 1.63 & 2.55 & 2.57 & 3.44 & 4.77 & 0.94 & 2.14 & 3.51 \\
      &95  & 5.44 & 7.70 & 10.91 & 0.80 & 1.63 & 2.56 & 2.64 & 3.45 & 4.84 & 0.94 & 2.14 & 3.52 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[htsb]
  \caption[Medium Size HD - Time per Iteration]{Medium Size HD - Time per Iteration in seconds.}
  \label{tab:medium_hd_all}
  \centering
  \begin{tabular}{lllll|l l l|l l l|l l l}
    \toprule
      & & \multicolumn{3}{c}{Julia} & \multicolumn{3}{c}{R} & \multicolumn{3}{c}{HyPer C++} & \multicolumn{3}{c}{HyPer LLVM}  \\
      & \emph{k} & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 \\
    \midrule
      \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{percentile}}}} & 50  & 11.46 & 15.69 & 21.67 & 9.43 & 15.48 & 23.77 & 16.39 & 34.12 & 59.40 & 10.42 & 22.11 & 38.80 \\
     & 90  & 11.51 & 15.81 & 21.69 & 9.50 & 15.50 & 23.79 & 16.77 & 36.11 & 59.53 & 10.44 & 22.12 & 38.84 \\
     & 95  & 11.65 & 15.83 & 21.70 & 9.64 & 15.58 & 23.79 & 17.29 & 42.15 & 59.57 & 10.45 & 22.13 & 38.84 \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{figure}[htsb]
  \centerline{
    \includegraphics[scale=0.4, trim="0cm 1cm 0cm 0cm"]{figures/charts/15M_all}
  }
  \caption[Medium Size - Time per Iteration]{Medium Size - Time per Iteration.}
  \label{fig:15M_all}
\end{figure}


\begin{figure}[htsb]
  \centerline{
    \includegraphics[scale=0.4, trim="0cm 1cm 0cm 0cm"]{figures/charts/15Mxhd_all}
  }
  \caption[Medium Size HD - Time per Iteration]{Medium Size HD - Time per Iteration.}
  \label{fig:15Mxhd_all}
\end{figure}



We perform the same experiment on the medium size and the medium size high dimensional data set.~\autoref{tab:medium_all} shows the result of the medium size data set. Again, the LLVM version is faster than the C++ version by a factor of 2.5, 1.6 and 1.4 for $k = 3, 10$ and $20$. Since the dataset has only four dimensions Julia performs poorly and is by a factor of 5.9, 3.6 and 3.1 slower compared to the LLVM version. As for the network data set R demonstrates the best performance and is faster than LLVM by a factor of 0.8, 0.8 and 0.7 for $k = 3, 10$ and 20. Again we depict the result as a boxplot in~\autoref{fig:15M_all}, where we observe a small variance for all used tools; only Julia has a few outliers for $k = 20$.
\\
In comparison,~\autoref{tab:medium_hd_all} and ~\autoref{fig:15Mxhd_all} depict the result for the medium size high dimensional data set. As before, the HyPer implementation does not perform ideally on high dimensions. For $k = 3$, the LLVM version is slower than R by a factor of 0.9 but faster than Julia by a factor of 1.1. However, as $k$ grows, R and Julia show slightly better results. Both show a speed up by a factor of 0.7 for $k = 10$, and 0.6 for $k = 20$, compared to the LLVM version. 
\\
Obviously the high dimensional data set takes more time per iteration having the same number of instances with a lot more dimensions. However, Julia shows a slow down by a factor of only 2 for all $k$'s, although the dimensions increase from 4 to 50; R shows a slow down by a factor of 12 for $k = 3$, and 10 for $k = 10$ and 20, the C++ version by a factor of 7, 10, 13 for $k = 3, 10$ and 20, and the LLVM version by a factor of 11 for all $k$'s. Julia performs again the best for high dimensional data. R and HyPer's LLVM operator behave very similar this time. An explanation is that the compilation time is not a limiting factor anymore for the LLVM implementation: For the high dimensional data set with 50,000 instances the compilation time was higher than the execution time. This time the data set consists of 15 million tuples therefore the poor compilation time does not affect the overall performance anymore.
\\
As final test we perform the same experiment on the large data set containing 150 million instances, as shown in~\autoref{tab:150M_all}. This time the HyPer LLVM implementation outnumbers all the other tools, the C++ version by a factor of 2.0, 1.5 and 1.4 for $k = 3, 10$ and 20, Julia by a factor of 3.6, 2.2, 2.0, respectively, and even R by a factor of 1.6, 1.2 and 1.1, respectively. For growing $k$, the R implementation comes very close to the HyPer LLVM version. This result is also depicted in the boxplot in~\autoref{fig:150M_all}.
\\
In conclusion, the experiments demonstrate that the HyPer k-Means operator can compete with state-of-the-art tools for clustering. Particularly the LLVM implementation demonstrates good performance and shows similar results as the R implementation of the k-Means algorithm. However, for high dimensional data sets both HyPer operators do not perform ideally and are outnumbered by Julia's implementation optimized for high dimensions. Here is potential for future optimization as we demonstrate in Section~\ref{section:parallel} with a parallel implementation. Yet, if we take into account that the k-Means algorithm runs on top of an entire database with all its overhead, these results are supporting further development and the implementation of other data mining algorithms as HyPer operators. 




\begin{table}[htsb]
  \caption[Large Size - Time per Iteration]{Large Size - Time per Iteration in seconds.}
  \label{tab:150M_all}
  \centering
  \begin{tabular}{l l l ll |l l l |l l l |l l l}
    \toprule
      && \multicolumn{3}{c}{Julia} & \multicolumn{3}{c}{R} & \multicolumn{3}{c}{HyPer C++} & \multicolumn{3}{c}{HyPer LLVM}  \\
      &\emph{k} & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 \\
    \midrule
      \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{percentile}}}} &50  & 65.62 & 90.37 & 132.39 & 29.47 & 48.21 & 72.40 & 37.29 & 62.01 & 92.60 & 18.38 & 40.49 & 67.25 \\
      &90  & 65.72 & 92.01 & 135.87 & 30.80 & 54.73 & 77.27 & 37.50 & 62.41 & 92.86 & 18.44 & 40.72 & 67.57 \\
      &95  & 65.78 & 92.27 & 137.21 & 33.08 & 55.02 & 77.97 & 37.52 & 62.72 & 92.95 & 18.46 & 40.72 & 67.63 \\
    \bottomrule
  \end{tabular}
\end{table}





\begin{figure}[htsb]
  \centerline{
    \includegraphics[scale=0.4, trim="0cm 1cm 0cm 0cm"]{figures/charts/150M_all}
  }
  \caption[Large Size - Time per Iteration]{Large Size - Time per Iteration.}
  \label{fig:150M_all}
\end{figure}



\section{Parallel Implementation}\label{section:parallel}


So far we have only covered the serial implementations of the HyPer k-Means operator: In Section~\ref{section:serial}  we figured out that the LLVM implementation outperforms the C++ version on all used data sets. In Section~\ref{section:performance} we compared the performance against Julia and R and showed that HyPer is able to compete with state-of-the-art technologies for data mining.
\\
However, as data sets grow serial execution takes more and more time, making real-time data mining almost impossible. Furthermore, we do not exploit the possibilities of modern database hardware and the advantages of multi-threaded execution of algorithms. To address these challenges we presented a parallel implementation of the HyPer k-Means operator in Section~\ref{section:parallel_implementation}. In this section we compare this parallel implementation with the LLVM version and the R implementation. Both tools demonstrate the best results on the medium size, the medium size high dimensional and the large data set. Julia and the C++ driven implementation are excluded in this section since their performance is either similar or slower to R and the LLVM implementation on the used data sets.

\begin{table}[htsb]
  \caption[Medium Size - Time per Iteration]{Medium Size - Time per Iteration in seconds.}
  \label{tab:medium_final}
  \centering
  \begin{tabular}{l l l ll |l l l |l l l }
    \toprule
      && \multicolumn{3}{c}{R} & \multicolumn{3}{c}{HyPer LLVM} & \multicolumn{3}{c}{HyPer Parallel}  \\
      &\emph{k} & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 \\
    \midrule
      \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{percentile}}}} &50  & 0.77 & 1.61 & 2.50 & 0.92 & 2.10 & 3.46 & 1.35 & 1.52 & 1.75 \\
      &90  & 0.79 & 1.63 & 2.55 & 0.94 & 2.14 & 3.51 & 1.38 & 1.53 & 1.84 \\
      &95  & 0.80 & 1.63 & 2.56 & 0.94 & 2.14 & 3.52 & 1.40 & 1.53 & 1.85 \\
    \bottomrule
  \end{tabular}
\end{table}

~\autoref{tab:medium_final} shows the result as the time per iteration for $k = 3, 10$ and 20. As before, the median, the 90th and 95th percentile are presented. For $k = 3$, we surprisingly observe that both the R and the LLVM version are faster than the parallel version. However, as $k$ grows, the parallel version outperforms R and the LLVM version. This circumstance is also shown in the bar chart in~\autoref{fig:final_15M}.
\\
The reason is that for the parallel execution the running time is almost independent of $k$ because the computation is performed on different, independent threads that are only slightly affected by an increase of $k$. However, if $k$ would even further grow while the number of cores stays constant, the performance might be affected also in the parallel execution mode. For a $k$ between 3 and 20 this does not happen and therefore, the parallel version outperforms R and LLVM. For R, the time per iteration grows by a factor of 2.1 from $k = 3$ to $k = 10$, and by a factor of 1.6 from $k = 10$ to $k = 20$. For LLVM we observe an increase by a factor of 2.3 and 1.6, respectively. In contrast, the parallel implementation grows only by factor of 1.1 and 1.2, respectively. In other words, R and the LLVM version grow by seconds as $k$ grows, while the parallel version grows insignificantly by around 200 milliseconds. 

\begin{figure}[htsb]
  \centering
  \includegraphics[scale=0.5, trim="0cm 1.5cm 0cm 0cm"]{figures/charts/final_15M}
  \caption[Medium Size - Time per Iteration]{Medium Size - Time per Iteration in seconds.}
  \label{fig:final_15M}
\end{figure}

~\autoref{tab:medium_hd_final} shows the same experiment on the medium high dimensional data set. For this data set the parallel version outperforms the other two implementations for all $k$'s, even for $k = 3$.~\autoref{fig:final_15_hd} depicts that the parallel version is again almost independent of $k$, in contrast to R and LLVM. R grows by a factor of 1.6 from $k = 3$ to $k = 10$, and 1.5 from $k = 10$ to $k = 20$; for LLVM we observe an increase by a factor of 2.1 and 1.8, respectively, and for the parallel version a factor of 1.4 and 1.4, respectively. 
\\
Even though the parallel version is not as independent of the cluster number $k$ as for the low dimensional data set, the increase in time is much lower. Additionally, a performance gain is achieved by the ability to handle high dimensional data better: From four dimensions to 50 dimensions, the parallel execution time is only affected by a factor of 4.3, 5.3 and 6.5 for $k = 3, 10$ and 20. R and LLVM are affected by a much larger factor: 12.3, 9.6 and 9.5 for R, and 11.3, 10.5 and 11.2 for the LLVM implementation. Again, the performance benefits from the parallel execution: Computing the distance from each data point to each center point is computationally expensive, even more when the instances consist of many dimensions. Then, a computation on many, independent threads speeds up the performance tremendously, as shown in this experiment. 


\begin{table}[htsb]
  \caption[Medium Size HD - Time per Iteration]{Medium Size HD - Time per Iteration in seconds.}
  \label{tab:medium_hd_final}
  \centering
  \begin{tabular}{l l l ll |l l l |l l l }
    \toprule
      && \multicolumn{3}{c}{R} & \multicolumn{3}{c}{HyPer LLVM} & \multicolumn{3}{c}{HyPer Parallel}  \\
      &k & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 \\
    \midrule
      \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{percentile}}}} &50  & 9.43 & 15.48 & 23.77 & 10.42 & 22.11 & 38.80 & 5.84 & 8.04 & 11.30 \\
      &90  & 9.50 & 15.50 & 23.79 & 10.44 & 22.12 & 38.84 & 5.88 & 8.26 & 11.35 \\
      &95  & 9.64 & 15.58 & 23.79 & 10.45 & 22.13 & 38.84 & 5.88 & 8.29 & 11.37 \\
    \bottomrule
  \end{tabular}
\end{table}



\begin{figure}[htsb]
  \centering
  \includegraphics[scale=0.5, trim="0cm 1.5cm 0cm 0cm"]{figures/charts/15Mxhd_final}
  \caption[Medium Size HD - Time per Iteration]{Medium Size HD - Time per Iteration.}
  \label{fig:final_15_hd}
\end{figure}

Finally,~\autoref{tab:large_final} shows the result of the same experiment on the large size data set. Again, the parallel version is the fastest. For $k = 3$, the parallel version is faster than R by a factor of 1.8 and for LLVM by a factor of 1.1. For $k = 10$ the factors are even larger, 2.4 for R and 2.0 for LLVM, and 3.0 and 2.8 for $k = 20$, respectively. Again $k$ affects the performance of the parallel version only slightly, therefore the speed up is increasing as $k$ increases.
\\
To conclude, the parallel version performs much better than the LLVM and R implementation. Except for the medium size data set and $k = 3$ it demonstrates always better results. However, since we are using only 16 cores even a higher speed up would be possible, in particular for larger data sets, more dimensions and a growing $k$. Furthermore, we are using the serial C++ version of the HyPer k-Means operator as foundation for our parallel implementation. As seen in the previous sections, this implementation performs poorly compared to the LLVM implementation: As in the serial implementation there are many function calls between the \texttt{runtime} and the \texttt{compile time system} also in the parallel version. Additionally, we are using high-level C++ constructs over the more efficient LLVM code generation. Therefore, in future implementations the LLVM version should be the building block of the parallel version which might lead to crucial performance improvements as observed already between the two serial implementations. And finally, only the computation of the distances is parallelized so far, while the recalculation of the cluster centers is still performed in serial. Therefore, we can expect even better results when we enhance the addressed shortcomings in future versions.


\begin{table}[htsb]
  \caption[Large Size - Time per Iteration]{Large Size - Time per Iteration in seconds.}
  \label{tab:large_final}
  \centering
  \begin{tabular}{l l l l l |l l l |l l l }
    \toprule
      && \multicolumn{3}{c}{R} & \multicolumn{3}{c}{HyPer LLVM} & \multicolumn{3}{c}{HyPer Parallel}  \\
      \emph{k} & 3 & 10 & 20 & 3 & 10 & 20 & 3 & 10 & 20 \\
    \midrule
      \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{percentile}}}} &50  & 29.47 & 48.21 & 72.40 & 18.38 & 40.49 & 67.25 & 16.71 & 19.79 & 24.03 \\
      &90  & 30.80 & 54.73 & 77.27 & 18.44 & 40.72 & 67.57 & 16.94 & 20.13 & 24.33 \\
      &95  & 33.08 & 55.02 & 77.97 & 18.46 & 40.72 & 67.63 & 17.01 & 20.17 & 24.36 \\
    \bottomrule
  \end{tabular}
\end{table}




\begin{figure}[htsb]
  \centering
  \includegraphics[scale=0.5, trim="0cm 1.5cm 0cm 0cm"]{figures/charts/final_150M}
  \caption[Large Size - Time per Iteration]{Large Size - Time per Iteration.}
  \label{fig:final_150M}
\end{figure}
